{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86c79f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Purpose: to process data files from lexisuni \n",
    "##### Author: Julia Cope\n",
    "##### Creation Date: 4/20/23\n",
    "##### Project: A2 NLP - capturing climate claims \n",
    "##### Inputs: 'C:/01_Pr/' \n",
    "##### Above contains a folder in which is the datasets from Nexis Uni Bulk \n",
    "##### Inputs: \n",
    "##### Inputs: \n",
    "##### Output: '03_Outputs/01_text_metadata.csv'\n",
    "##### Output: \n",
    "##### Output: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43123ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ea8f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Module\n",
    "import os\n",
    "\n",
    "# Folder Path\n",
    "#path = \"C:\\\\01_Pr\"\n",
    "path = \"C:\\\\2P\"\n",
    "\n",
    "# Change the directory\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9c5d336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\2P'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## print working direction\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "125cd6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/5186839/python-replace-with \n",
    "\n",
    "#def get_df():\n",
    "def get_df(path,s_string):\n",
    "    #initialize empty df \n",
    "    #metadata_df = pd.DataFrame()\n",
    "    metadata_df = []\n",
    "    #data = []\n",
    "    text_data_array = []\n",
    "    text_filename_array = []\n",
    "    column_names = ['Filename', 'Publication', 'Section', 'Date', 'Title', 'Author',\n",
    "       'LNID']\n",
    "    #rootdir = './'\n",
    "    #rootdir = \".\\\\\"\n",
    "    #rootdir_full = 'C:/Users/julia/OneDrive/Documents/Penn 2022-2023/nlp project/press releases 1/julia.cope@asc.upenn.edu-b1720-s2023-03-16/'\n",
    "    i = 0 \n",
    "    #starts_with_string = rootdir_full + 'term_press_release'\n",
    "    #starts_with_string = path + 'term_press_release'\n",
    "    starts_with_string = path + s_string\n",
    "\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            #print(\"In folder: \" + subdir)\n",
    "            if subdir.startswith(starts_with_string):\n",
    "                if file.endswith('.csv'):\n",
    "                    #print(\"In folder: \" + subdir)\n",
    "                    csv_path = os.path.join(subdir, file)\n",
    "                    csv_path = str(csv_path).replace(os.path.sep, '/')\n",
    "                    #csv_path = str(subdir) + '/' + str(file)\n",
    "                    #print(str(csv_path))\n",
    "                    #csv_path\n",
    "                    #print(os.path.join(subdir, file))\n",
    "                    with open(str(csv_path), newline='',encoding=\"utf8\") as csvfile:\n",
    "                        temp_data1 = list(csv.reader(csvfile))\n",
    "   \n",
    "                    metadata_df.extend(temp_data1[1:])\n",
    "                    #temp_meta_df = pd.read_csv(str(csv_path))\n",
    "                    #metadata_df = pd.concat([metadata_df, temp_meta_df])\n",
    "                    #print(metadata_df)\n",
    "                if file.endswith('.txt'):\n",
    "                    i = i + 1 \n",
    "                    ## open file \n",
    "                    ## read the data \n",
    "                    ## put it in the numpy array \n",
    "                    txt_path = str(subdir).replace(os.path.sep, '/') + '/' + str(file)\n",
    "                    \n",
    "                    #print(txt_path)\n",
    "                    text_file = open(txt_path, \"r\",encoding=\"utf8\")\n",
    "                    #read whole file to a string\n",
    "                    data = text_file.read()\n",
    "                    #close file\n",
    "                    text_file.close()\n",
    "                    \n",
    "                    ##append to array data \n",
    "                    text_data_array.append(data)\n",
    "                    ##append to filename array \n",
    "                    text_filename_array.append(str(file))\n",
    "                    \n",
    "    \n",
    "\n",
    "\n",
    "    #merge dataframes on Filename and filename\n",
    "    #https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html\n",
    "    #text_arr = np.column_stack((text_filename_array,text_data_array))\n",
    "    d = dict(zip(text_filename_array, text_data_array))\n",
    "\n",
    "    #d = {'Filename': text_filename_array, 'text': text_data_array}\n",
    "    text_filename_array=[]\n",
    "    text_data_array = []\n",
    "    \n",
    "    \n",
    "    ## get metadata as a df\n",
    "    metadata_df = pd.DataFrame(metadata_df)\n",
    "    metadata_df.columns = column_names\n",
    "    ## merge dfs\n",
    "    metadata_df[\"text\"] = metadata_df[\"Filename\"].map(d)\n",
    "    \n",
    "    \n",
    "    #articles_df_output = metadata_df.merge(text_df, on = \"Filename\", how=\"outer\")\n",
    "    print(\"There are \" + str(i) + \" articles\")\n",
    "    \n",
    "    return metadata_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed017f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a09c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dc05af5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2124\\2030636562.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0ms_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'term_press_release'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mPR01_df\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mget_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2124\\4062746759.py\u001b[0m in \u001b[0;36mget_df\u001b[1;34m(path, s_string)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;31m#print(\"In folder: \" + subdir)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\os.py\u001b[0m in \u001b[0;36m_walk\u001b[1;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[0;32m    416\u001b[0m             \u001b[1;31m# above.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfollowlinks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mislink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 418\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m_walk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopdown\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollowlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;31m# Recurse into sub-directories\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\os.py\u001b[0m in \u001b[0;36m_walk\u001b[1;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[0;32m    416\u001b[0m             \u001b[1;31m# above.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfollowlinks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mislink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 418\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m_walk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopdown\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollowlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;31m# Recurse into sub-directories\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\os.py\u001b[0m in \u001b[0;36m_walk\u001b[1;34m(top, topdown, onerror, followlinks)\u001b[0m\n\u001b[0;32m    365\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m                     \u001b[0mentry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscandir_it\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#path = 'C:/Users/julia/OneDrive/Documents/Penn 2022-2023/nlp project/press releases 1/julia.cope@asc.upenn.edu-b1720-s2023-03-16/'\n",
    "# 30 seconds for 1300 articles\n",
    "## 10 minutes for 30,000 articles\n",
    "#path = 'C:/01_Pr/'\n",
    "path = 'C:/2P/Che/'\n",
    "\n",
    "s_string = 'term_press_release'\n",
    "PR01_df =  get_df(path,s_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3923974",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1611 articles\n"
     ]
    }
   ],
   "source": [
    "## a few minutes\n",
    "path = 'C:/2P/Mar1/'\n",
    "s_string = 'term_press_release'\n",
    "PR03_df = get_df(path,s_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cbd9863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Filename    Publication  \\\n",
      "0     business-wire-requisite_s-emerge-catalog-256e9...  Business Wire   \n",
      "1     business-wire-exxonmobil-announces-canela-2572...  Business Wire   \n",
      "2     business-wire-procuri_s-2003-success-2577673e-...  Business Wire   \n",
      "3     business-wire-phillips-to-build-257a762c-dfdf-...  Business Wire   \n",
      "4     business-wire-chemoil-selects-triple-257e605c-...  Business Wire   \n",
      "...                                                 ...            ...   \n",
      "1606  pr-newswire-mplx-lp-reports-3eeb5c70-dfdf-11ed...    PR Newswire   \n",
      "1607  pr-newswire-industry-support-for-3eef8a52-dfdf...    PR Newswire   \n",
      "1608  pr-newswire-marathon-petroleum-corp.-3ef1d938-...    PR Newswire   \n",
      "1609  pr-newswire-enbridge-appoints-new-3ef564e0-dfd...    PR Newswire   \n",
      "1610  pr-newswire-marathon-petroleum-corp.-3ef989d0-...    PR Newswire   \n",
      "\n",
      "     Section        Date                                              Title  \\\n",
      "0             2002-04-25  Requisite's eMerge Catalog Content Management ...   \n",
      "1             2004-04-20  ExxonMobil Announces Canela Discovery in Deepw...   \n",
      "2             2004-02-19  Procuri's 2003 Success Marked by Significant C...   \n",
      "3             2002-07-09   Phillips to Build S Zorb Units at Two Refineries   \n",
      "4             2001-12-07  Chemoil Selects Triple Point Technology's Temp...   \n",
      "...      ...         ...                                                ...   \n",
      "1606          2020-01-29  MPLX LP Reports Fourth-Quarter and Full-Year F...   \n",
      "1607          2022-01-20  Industry Support for Large-Scale Carbon Captur...   \n",
      "1608          2021-07-28  Marathon Petroleum Corp. Announces Quarterly D...   \n",
      "1609          2020-02-14        Enbridge Appoints New Director to its Board   \n",
      "1610          2020-03-04  Marathon Petroleum Corp. Establishes Greenhous...   \n",
      "\n",
      "     Author                          LNID  \\\n",
      "0            45NW-G850-010G-028C-00000-00   \n",
      "1            4C6P-8270-01KN-104Y-00000-00   \n",
      "2            4BRP-1YR0-01KN-12V1-00000-00   \n",
      "3            467W-8C30-010G-03YB-00000-00   \n",
      "4            44M7-GW10-010G-02G5-00000-00   \n",
      "...     ...                           ...   \n",
      "1606         5Y3F-29V1-DXP3-R11S-00000-00   \n",
      "1607         64K7-CCK1-DXP3-R4FM-00000-00   \n",
      "1608         637R-N8S1-DXP3-R283-00000-00   \n",
      "1609         5Y6V-JJ81-DXP3-R4DP-00000-00   \n",
      "1610         5YBR-69H1-DXP3-R0GD-00000-00   \n",
      "\n",
      "                                                   text  \n",
      "0     Global Automotive Exchange to Achieve Signific...  \n",
      "1     Exxon Mobil Corporation announced today that i...  \n",
      "2     Strategic Sourcing Provider Signs 43 New Long-...  \n",
      "3     Phillips Petroleum Company (NYSE:P) today anno...  \n",
      "4     Triple Point Technology, Inc. (www.tpt.com), t...  \n",
      "...                                                 ...  \n",
      "1606  PR Newswire\\n\\nReported fourth-quarter net los...  \n",
      "1607  PR Newswire\\n\\nThree additional companies have...  \n",
      "1608  PR Newswire\\n\\nThe board of directors of Marat...  \n",
      "1609  PR Newswire\\n\\nEnbridge Inc. (TSX:ENB) (NYSE:E...  \n",
      "1610  PR Newswire\\n\\nMarathon Petroleum Corp. (NYSE:...  \n",
      "\n",
      "[1611 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(PR03_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "022109b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3894 articles\n"
     ]
    }
   ],
   "source": [
    "path = 'C:/2P/Val/'\n",
    "s_string = 'term_press_release'\n",
    "PR04_df = get_df(path,s_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3b54b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "##save as a csv file \n",
    "outputdf =   PR04_df                \n",
    "#outputdf = pd.concat([ PR03_df, PR04_df])\n",
    "#outputdf = pd.concat([PR01_df, PR03_df_2017])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f67a622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "920e641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\julia\\\\OneDrive\\\\Desktop\\\\NLP\"\n",
    "\n",
    "# Change the directory\n",
    "os.chdir(path)\n",
    "outputdf.to_csv('03_Outputs/01_valero_metadata.csv',index=False)\n",
    "\n",
    "#outputdf.to_csv('03_Outputs/01_text_metadata.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31578a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
